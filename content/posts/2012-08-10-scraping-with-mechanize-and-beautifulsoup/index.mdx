---
layout: post
title: 'Scraping with Mechanize and BeautifulSoup'
date: 2012-08-10
redirect_from: 
            - /blog/scraping-with-mechanize-and-beautifulsoup/swizec/5039
categories: 'Uncategorized'
author: Swizec Teller
hero: ./img/wikipedia-commons-thumb-0-07-Parma_-_scraping_the_deck.jpg-800px-Parma_-_scraping_the_deck.jpg
---
![Scraping](./img/wikipedia-commons-thumb-0-07-Parma_-_scraping_the_deck.jpg-800px-Parma_-_scraping_the_deck.jpg "Scraping")

Scraping is one of those annoying little things that will never be solved for the general case. Sometimes you want to [extract articles](http://www.zemanta.com/fruitblog/the-uncanny-valley-of-web-scraping/), other times you're looking for data in organized tables ... and sometimes it's all hidden behind a form with [cross-site request forgery](http://en.wikipedia.org/wiki/Cross-site_request_forgery "Cross-site request forgery")protection (csrf).



And it's never _actually_ organized. Even with the best of websites, I don't think I've ever encountered a scraping job that couldn't be described as _"A small and simple general model with heaps upon piles of annoying little exceptions"_



At best scraping a bunch of data from a website is a somewhat fiddly job, at worst you'll be wishing you'd done it manually and be done with it.



When you're done, you will lie awake at night, praying to the gods of the internets, hoping nobody sneezes at the [HTML](http://en.wikipedia.org/wiki/HTML "HTML") of that page.

## Mechanize

You won't get away from the fiddliness, but there's a lot you can do to make the job more palatable.



For starters - ditch manually taking care of submitting forms, hauling cookies around, holding history, sending referrers, using a good user-agent, following redirects and so on and on.



Submitting a form usually goes like this:

1. Go to page
2. View source
3. Find form
4. Note the _action_ url
5. Make a note of the field names
6. Make sure honeypot fields will be handled properly
7. Write a few lines of code to prepare data for submission
8. Submit to the correct url

Then you discover the website uses csrf protection and you have to make a script that will go to the form address, parse the form, find the csrf field, hold the proper cookie and so on ...



Pain in the ass.

```
\import urllib, urllib2

req = urllib2.Request("http://example.com/form/submit/url"
                      data=urllib.urlencode({'field1': 'value',
                                             'field2': 'value',
                                             'filed3': 'value'}),
                      headers={'User-Agent': 'Mozilla something',
                               'Cookie': 'name=value; name2=value2'})
response = urllib2.urlopen(req)
# do something with response
```

With [Mechanize](http://wwwsearch.sourceforge.net/mechanize/development.html) the process is much simpler:

1. Go to page
2. View source
3. Find form
4. Note an identifier for the form
5. Decide which fields you want to manipulate
6. Write some code

The beautiful thing is, mechanize will automatically handle csrf fields and most other popular forms of preventing bots doing their dirty business all over a website.

```
\import mechanize

browser.open('http://example.com/form/')
browser.select_form(name='the_form')
browser['field1'] = 'value'
browser['field2'] = 'value'
browser['field3'] = 'value'
browser.submit()

# use browser to click on stuff
# or browser.response() to get the raw response
```

Now isn't that much easier and cleaner?



The cool thing about Mechanize is that it also lets you do a lot of browsing around. _browser.links_ gives you all the links on a page, _browser.forms_ all the forms and so on. You can even use _browser.follow_link_ to naturally walk around the whole website like a user might.



Which is very useful when you're handling websites that either don't want or don't expect bots.

## BeautifulSoup

Ok, now we can get to the data. But how do we get the data itself?



Unfortunately this is the fiddly part of the process and there isn't much you can do about that. Your best bet is using BeautifulSoup to at least make the process of handling poorly written HTML without a big fuss. It will even make sure everything is unicode. Win!



At least [BeautifulSoup](http://www.crummy.com/software/BeautifulSoup/) makes browsing [DOM](http://en.wikipedia.org/wiki/Document_Object_Model "Document Object Model") a breeze:

```
from bs4 \import BeautifulSoup

soup = BeautifulSoup(browser.response().read())

body_tag = soup.body
all_paragraphs = soup.find_all('p')
logo_img = soup.find('header').find('div', id="logo").img

# and so on depending on what you need
```

That's it.



The next time you have to scrape some data off a website I suggest using Mechanize and BeautifulSoup. That way you can worry about the fiddly bits, not the infrastructure.

###### Related articles

- [![](http://i.zemanta.com/noimg_95_80_80.jpg)](http://stackoverflow.com/questions/11284643/python-high-memory-usage-with-beautifulsoup)[Python high memory usage with BeautifulSoup](http://stackoverflow.com/questions/11284643/python-high-memory-usage-with-beautifulsoup)
- [![](http://i.zemanta.com/92286111_80_80.jpg)](http://stackoverflow.com/questions/10867253/screen-scraping-images-ie-firefox-page-info-google-images)[Screen scraping images (ie. Firefox Page Info / Google Images)](http://stackoverflow.com/questions/10867253/screen-scraping-images-ie-firefox-page-info-google-images)
- [![](http://i.zemanta.com/noimg_50_80_80.jpg)](http://coffeeonthekeyboard.com/csrf-cross-site-request-forgeries-basic-security-part-3-747/)[James Socol: CSRF: Cross-Site Request Forgeries - Basic Security Part 3](http://coffeeonthekeyboard.com/csrf-cross-site-request-forgeries-basic-security-part-3-747/)
- [![](http://i.zemanta.com/noimg_70_80_80.jpg)](http://www.blog.pythonlibrary.org/2012/06/08/python-101-how-to-submit-a-web-form/)[Python 101: How to submit a web form](http://www.blog.pythonlibrary.org/2012/06/08/python-101-how-to-submit-a-web-form/)
- [![](http://i.zemanta.com/105660603_80_80.jpg)](http://searchenginewatch.com/article/2197592/Extreme-Negative-SEO-Know-Your-Vulnerabilities)[Extreme Negative SEO: Know Your Vulnerabilities](http://searchenginewatch.com/article/2197592/Extreme-Negative-SEO-Know-Your-Vulnerabilities)

[![Enhanced by Zemanta](http://img.zemanta.com/zemified_e.png?x-id=85cda396-14a6-4cc1-bbb8-12673fd5c287)](http://www.zemanta.com/?px "Enhanced by Zemanta")